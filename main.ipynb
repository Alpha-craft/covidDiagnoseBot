{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the library, this are the library we use\r\n",
    "\r\n",
    "*note: aiogram is telegram bot framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\r\n",
    "import numpy\r\n",
    "import pandas\r\n",
    "import random\r\n",
    "import math\r\n",
    "import time\r\n",
    "import datetime\r\n",
    "import operator\r\n",
    "import requests\r\n",
    "import pytz\r\n",
    "\r\n",
    "import aiogram.utils.markdown as md\r\n",
    "from aiogram.types import message\r\n",
    "from aiogram import Bot, Dispatcher, executor, types\r\n",
    "from aiogram.contrib.fsm_storage.memory import MemoryStorage\r\n",
    "from aiogram.dispatcher import FSMContext\r\n",
    "from aiogram.dispatcher.filters import Text\r\n",
    "from aiogram.dispatcher.filters.state import State, StatesGroup\r\n",
    "from aiogram.types import ParseMode\r\n",
    "from aiogram.utils import executor\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\r\n",
    "from sklearn.svm import SVR\r\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\r\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we create sastrawi stemmer object and read response-sentence csv with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_factory = StopWordRemoverFactory()\r\n",
    "sw_remover = sw_factory.create_stop_word_remover()\r\n",
    "\r\n",
    "stem_factory = StemmerFactory()\r\n",
    "stemmer = stem_factory.create_stemmer()\r\n",
    "\r\n",
    "rsp_list = pandas.read_csv(\"response.csv\")\r\n",
    "snt_list = pandas.read_csv(\"sentence.csv\")\r\n",
    "syn_list = pandas.read_csv(\"sinonim.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Main echo functions ===\r\n",
    "We're gonna skip the aiogram code a bit and just went straight to main text and data proccessing\r\n",
    "\r\n",
    "So basically the main code to detect user text input is like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\r\n",
    "storage = MemoryStorage()\r\n",
    "\r\n",
    "bot = Bot(token=\"Bot_token_here..\")\r\n",
    "dp = Dispatcher(bot, storage=storage)\r\n",
    "\r\n",
    "@dp.message_handler()\r\n",
    "async def echo(message: types.Message):\r\n",
    "    bot_respon = [] # to save responses\r\n",
    "    detected_intent = [] # to save intents\r\n",
    "    pesan = message.text.lower() # user input\r\n",
    "    kata = pesan.split() # user words (splitted)\r\n",
    "\r\n",
    "\r\n",
    "    get_responses(kata, detected_intent, bot_respon)      \r\n",
    "    bot_respon += get_covid_info( stemmer.stem(synonymize(pesan)) )\r\n",
    "\r\n",
    "    # iterate the bot_respon (if not null)\r\n",
    "    for item in bot_respon:\r\n",
    "        if item is not None:          \r\n",
    "            await message.answer(item) # send the response to user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a bunch of made function, so lets head back and see the functions code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make a function to get responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(words, intents, current_responses):\r\n",
    "  for word in words: # iterate user input\r\n",
    "      for sentence in snt_list.itertuples(): # convert senctence list and iterate the sentences\r\n",
    "\r\n",
    "          # if stemmed input match the sentence in csv and same intent is not exist\r\n",
    "          if stemmer.stem(sentence.Sentence) == stemmer.stem(word) and sentence.Intent not in intents:\r\n",
    "            # add both response and intent\r\n",
    "            current_responses = add_respon(current_responses, sentence.Intent)            \r\n",
    "            intents += [ sentence.Intent ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is the add_respon function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "def add_respon(responses, key):\r\n",
    "  # search response that match the key (intent)\r\n",
    "  getResponse = rsp_list[rsp_list[\"Intent\"] == key].to_records(index=False)\r\n",
    "\r\n",
    "  #if not empty the add the response\r\n",
    "  if not empty(getResponse):\r\n",
    "    responses += [ random.choice(getResponse)[1] ]\r\n",
    "\r\n",
    "  return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty function's basically just check if array lenght is not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty(lists):\r\n",
    "  if len(lists) == 0:\r\n",
    "    return True\r\n",
    "\r\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also have synonymize to replace the same intent words into one same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonymize(words):\r\n",
    "  sinonim_list = syn_list.to_records(index=False)\r\n",
    "  result = words.lower()\r\n",
    "\r\n",
    "  for sinonims in sinonim_list:\r\n",
    "    for item in str(sinonims[1]).split(','):\r\n",
    "      result = result.replace(item, f\"{sinonims[0]} \", 1)    \r\n",
    "\r\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Covid Info ===\r\n",
    "\r\n",
    "Next is get_covid_info functions\r\n",
    "\r\n",
    "This function goals is to predict user requested topic of covid related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covid_info(user_input):\r\n",
    "  response = None\r\n",
    "  article = open(\"article.txt\", \"r\")\r\n",
    "\r\n",
    "  paragraph = article.read().split(\"\\n\\n\\n\")\r\n",
    "  tokens = [ stemmer.stem(sw_remover.remove(synonymize(token))) for token in paragraph ]\r\n",
    "\r\n",
    "  tokens.append(user_input)\r\n",
    "  vectorized = CountVectorizer().fit_transform(tokens)\r\n",
    "  similarity = cosine_similarity(vectorized[-1], vectorized)\r\n",
    "  similarity_list = similarity.flatten()\r\n",
    "  index = get_similarity_index(similarity_list)\r\n",
    "  index = index[1:]\r\n",
    "\r\n",
    "  for x in range(len(index)):\r\n",
    "    if similarity_list[index[x]] > 0.2:\r\n",
    "      response = f\"{paragraph[index[x]]}\\n\\n\"\r\n",
    "      break\r\n",
    "\r\n",
    "  tokens.remove(user_input)\r\n",
    "\r\n",
    "  return [ response ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to reshape the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_index(lists):\r\n",
    "  lens = len(lists)\r\n",
    "  list_index = list(range(0, lens))\r\n",
    "\r\n",
    "  for x in range(lens):\r\n",
    "    for y in range(lens):\r\n",
    "      if lists[list_index[x]] > lists[list_index][y]:\r\n",
    "        swap = list_index[x]\r\n",
    "        list_index[x] = list_index[y]\r\n",
    "        list_index[y] = swap\r\n",
    "\r\n",
    "  return list_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Predict covid-19 case ===\r\n",
    "\r\n",
    "This is a function to display and predict covid-19 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covid_stats():\r\n",
    "    now = datetime.datetime.now(pytz.timezone('Asia/Jakarta'))\r\n",
    "    head = []\r\n",
    "    data = []\r\n",
    "    start = ''\r\n",
    "\r\n",
    "    lastweek = now - datetime.timedelta(days=now.weekday(), weeks=1)\r\n",
    "\r\n",
    "    for x in range(0, 7):\r\n",
    "        timey = lastweek + datetime.timedelta(days=x)\r\n",
    "        d = timey.strftime('%d')\r\n",
    "        m = timey.strftime('%B')\r\n",
    "        y = timey.strftime('%Y')\r\n",
    "\r\n",
    "        get_covid = requests.get(f\"https://apicovid19indonesia-v2.vercel.app/api/indonesia/provinsi/harian?year={y}&date={d}&month={m}\")      \r\n",
    "        covid_info = get_covid.json() \r\n",
    "    \r\n",
    "        if x == 0:\r\n",
    "          start = timey.strftime('%d-%b')\r\n",
    "\r\n",
    "        head += [ timey.strftime('%d-%b') ]\r\n",
    "        data += [ covid_info['data'][0]['cur_total'] ]\r\n",
    "\r\n",
    "    confirmed = pandas.DataFrame([data], columns = head)\r\n",
    "    dates = confirmed.keys()\r\n",
    "    cases = []      \r\n",
    "\r\n",
    "    for i in dates:\r\n",
    "        cases.append(confirmed[i].sum())\r\n",
    "\r\n",
    "    days_since = numpy.array([i for i in range(len(dates))]).reshape(-1, 1)\r\n",
    "    cases = numpy.array(cases).reshape(-1, 1)\r\n",
    "\r\n",
    "    days_in_future = 3\r\n",
    "    future_forcast = numpy.array([i for i in range(len(dates)+days_in_future)]).reshape(-1, 1)\r\n",
    "\r\n",
    "    start_date = datetime.datetime.strptime(start, '%d-%b')\r\n",
    "    future_forcast_dates = []\r\n",
    "    cases_date = []\r\n",
    "    for i in range(len(future_forcast)):\r\n",
    "        future_forcast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%d-%b'))\r\n",
    "\r\n",
    "    X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(days_since, cases, test_size=0.15, shuffle=False) \r\n",
    "\r\n",
    "    kernel = ['poly', 'sigmoid', 'rbf']\r\n",
    "    c = [0.01, 0.1, 1, 10, 100]\r\n",
    "    gamma = [0.01, 0.1, 10]\r\n",
    "    epsilon = [0.01, 0.1, 10]\r\n",
    "    shrinking = [True, False]\r\n",
    "    svm_parameters = {\r\n",
    "      'kernel': kernel, \r\n",
    "      'C': c, \r\n",
    "      'gamma' : gamma, \r\n",
    "      'epsilon': epsilon, \r\n",
    "      'shrinking' : shrinking\r\n",
    "    }\r\n",
    "\r\n",
    "    svm = SVR()\r\n",
    "    svm_search = RandomizedSearchCV(svm, svm_parameters, scoring='neg_mean_squared_error', cv=3, return_train_score=True, n_jobs=1, n_iter=45, verbose=2)  \r\n",
    "    svm_search.fit(X_train_confirmed, y_train_confirmed.ravel())\r\n",
    "\r\n",
    "    svm_search.best_params_\r\n",
    "\r\n",
    "    svm_confirmed = svm_search.best_estimator_\r\n",
    "    svm_pred = svm_confirmed.predict(future_forcast)\r\n",
    "\r\n",
    "    x = ''\r\n",
    "    y = ''\r\n",
    "    for i in cases:\r\n",
    "        x += f'{round(i[0])},'\r\n",
    "    for i in svm_pred:\r\n",
    "        y += f'{round(i)},'\r\n",
    "    x = x.rstrip(',')\r\n",
    "    y = y.rstrip(',')\r\n",
    "\r\n",
    "    url = f\"https://image-charts.com/chart?cht=lc&chd=a:|{ x }|{ y }&chdl=Prediksi|Terkonfirmasi&chxl=0:|{ '|'.join(future_forcast_dates) }|1:||1000|2000|3000|4000|5000|&chs=900x500&chco=3072F3,ff0000&chdlp=t&chls=2,4,1&chm=s,000000,0,-1,5|s,000000,1,-1,5&chxt=x,y\"\r\n",
    "\r\n",
    "    return {\r\n",
    "      \"img_url\": url,\r\n",
    "      \"confirmed\": cases, \r\n",
    "      \"prediction\": svm_pred,\r\n",
    "      \"date\": future_forcast_dates        \r\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python397jvsc74a57bd08da01e5a71448ea74f54d88afa8911010d1d12e23bc7e103d40d5def4a09152c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "8da01e5a71448ea74f54d88afa8911010d1d12e23bc7e103d40d5def4a09152c"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}