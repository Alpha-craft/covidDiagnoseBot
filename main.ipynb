{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the library, this are the library we use\r\n",
    "\r\n",
    "*note: aiogram is telegram bot framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\r\n",
    "import numpy\r\n",
    "import pandas\r\n",
    "import requests as req\r\n",
    "\r\n",
    "import aiogram.utils.markdown as md\r\n",
    "from aiogram.types import message\r\n",
    "from aiogram import Bot, Dispatcher, executor, types\r\n",
    "from aiogram.contrib.fsm_storage.memory import MemoryStorage\r\n",
    "from aiogram.dispatcher import FSMContext\r\n",
    "from aiogram.dispatcher.filters import Text\r\n",
    "from aiogram.dispatcher.filters.state import State, StatesGroup\r\n",
    "from aiogram.types import ParseMode\r\n",
    "from aiogram.utils import executor\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we create sastrawi stemmer object and read response-sentence csv with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\r\n",
    "stemmer = factory.create_stemmer()\r\n",
    "rsp_list = pandas.read_csv(\"response.csv\")\r\n",
    "snt_list = pandas.read_csv(\"sentence.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Main echo functions ===\r\n",
    "We're gonna skip the aiogram code and just went straight to main text and data proccessing\r\n",
    "\r\n",
    "So basically the main code to detect user text input is like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.message_handler()\r\n",
    "async def echo(message: types.Message):\r\n",
    "    bot_respon = [] # to save responses\r\n",
    "    detected_intent = [] # to save intents\r\n",
    "    pesan = message.text.lower() # user input\r\n",
    "    kata = pesan.split() # user words (splitted)\r\n",
    "\r\n",
    "\r\n",
    "    get_responses(kata, detected_intent, bot_respon)      \r\n",
    "    bot_respon += get_covid_info( stemmer.stem(synonymize(pesan)) )\r\n",
    "\r\n",
    "    # iterate the bot_respon (if not null)\r\n",
    "    for item in bot_respon:\r\n",
    "        if item is not None:          \r\n",
    "            await message.answer(item) # send the response to user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a bunch of made function, so lets head back and see the functions code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make a function to get responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(words, intents, current_responses):\r\n",
    "  for word in words: # iterate user input\r\n",
    "      for sentence in snt_list.itertuples(): # convert senctence list and iterate the sentences\r\n",
    "\r\n",
    "          # if stemmed input match the sentence in csv and same intent is not exist\r\n",
    "          if stemmer.stem(sentence.Sentence) == stemmer.stem(word) and sentence.Intent not in intents:\r\n",
    "            # add both response and intent\r\n",
    "            current_responses = add_respon(current_responses, sentence.Intent)            \r\n",
    "            intents += [ sentence.Intent ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is the add_respon function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "def add_respon(responses, key):\r\n",
    "  # search response that match the key (intent)\r\n",
    "  getResponse = rsp_list[rsp_list[\"Intent\"] == key].to_records(index=False)\r\n",
    "\r\n",
    "  #if not empty the add the response\r\n",
    "  if not empty(getResponse):\r\n",
    "    responses += [ random.choice(getResponse)[1] ]\r\n",
    "\r\n",
    "  return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty function's basically just check if array lenght is not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty(lists):\r\n",
    "  if len(lists) == 0:\r\n",
    "    return True\r\n",
    "\r\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also have synonymize to replace the same intent words into one same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonymize(words):\r\n",
    "  sinonim_list = syn_list.to_records(index=False)\r\n",
    "  result = words.lower()\r\n",
    "\r\n",
    "  for sinonims in sinonim_list:\r\n",
    "    for item in str(sinonims[1]).split(','):\r\n",
    "      result = result.replace(item, f\"{sinonims[0]} \", 1)    \r\n",
    "\r\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Covid Info ===\r\n",
    "\r\n",
    "Next is get_covid_info functions\r\n",
    "\r\n",
    "This function goals is to predict user requested topic of covid related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covid_info(user_input):\r\n",
    "  response = None\r\n",
    "  article = open(\"article.txt\", \"r\")\r\n",
    "\r\n",
    "  paragraph = article.read().split(\"\\n\\n\\n\")\r\n",
    "  tokens = [ stemmer.stem(sw_remover.remove(synonymize(token))) for token in paragraph ]\r\n",
    "\r\n",
    "  tokens.append(user_input)\r\n",
    "  vectorized = CountVectorizer().fit_transform(tokens)\r\n",
    "  similarity = cosine_similarity(vectorized[-1], vectorized)\r\n",
    "  similarity_list = similarity.flatten()\r\n",
    "  index = get_similarity_index(similarity_list)\r\n",
    "  index = index[1:]\r\n",
    "\r\n",
    "  for x in range(len(index)):\r\n",
    "    if similarity_list[index[x]] > 0.2:\r\n",
    "      response = f\"{paragraph[index[x]]}\\n\\n\"\r\n",
    "      break\r\n",
    "\r\n",
    "  tokens.remove(user_input)\r\n",
    "\r\n",
    "  return [ response ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the detail what this functions do!\r\n",
    "\r\n",
    "First define null response var and read the article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = None\r\n",
    "article = open(\"article.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the article into a paragraph\r\n",
    "\r\n",
    "And then we make tokens (a splited stemmed and simplified text) from the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = article.read().split(\"\\n\\n\\n\")\r\n",
    "tokens = [ stemmer.stem(sw_remover.remove(synonymize(token))) for token in paragraph ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we add the user input to the tokens and turn it into numpy vector, get the most similarity and turn it into flat list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.append(user_input)\r\n",
    "vectorized = CountVectorizer().fit_transform(tokens)\r\n",
    "similarity = cosine_similarity(vectorized[-1], vectorized)\r\n",
    "similarity_list = similarity.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the problem is the shape of list is not quite right to iterate, so we gonna rotate the list index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_index(lists):\r\n",
    "  lens = len(lists)\r\n",
    "  list_index = list(range(0, lens))\r\n",
    "\r\n",
    "  for x in range(lens):\r\n",
    "    for y in range(lens):\r\n",
    "      if lists[list_index[x]] > lists[list_index][y]:\r\n",
    "        swap = list_index[x]\r\n",
    "        list_index[x] = list_index[y]\r\n",
    "        list_index[y] = swap\r\n",
    "\r\n",
    "  return list_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_similarity_index(similarity_list)\r\n",
    "index = index[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then iterate the index of similarity list and compare the similarity value with your desired accuracy (we set the accuracy to 0.2)\r\n",
    "\r\n",
    "And lastly return the response (the matched paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(index)):\r\n",
    "  if similarity_list[index[x]] > 0.2:\r\n",
    "    response = f\"{paragraph[index[x]]}\\n\\n\"\r\n",
    "    break\r\n",
    "\r\n",
    "tokens.remove(user_input)\r\n",
    "\r\n",
    "return [ response ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Predict covid-19 case ===\r\n",
    "\r\n",
    "This is a function to display and predict covid-19 case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First defined what time is now and set the timezone.. also define some var to use later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now(pytz.timezone('Asia/Jakarta'))\r\n",
    "\r\n",
    "head = []\r\n",
    "data = []\r\n",
    "start = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step: iterate from 1 - 8\r\n",
    "\r\n",
    "subtract the \"now\" var for every iterable, get formatted time string, and do a request at \"apicovid19indonesia\" with the defined formatted time\r\n",
    "\r\n",
    "save the last iterate to \"start\" variable as a formatted string (now var contains the start date)\r\n",
    "\r\n",
    "add formatted string as a head for every iterate\r\n",
    "\r\n",
    "and lastly add the cases total as the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1, 8):\r\n",
    "  timey = now - datetime.timedelta(days=x)\r\n",
    "  d = timey.strftime('%d')\r\n",
    "  m = timey.strftime('%B')\r\n",
    "  y = timey.strftime('%Y')\r\n",
    "\r\n",
    "  get_covid = requests.get(f\"https://apicovid19indonesia-v2.vercel.app/api/indonesia/provinsi/harian?year={y}&date={d}&month={m}\")\r\n",
    "  covid_info = get_covid.json() \r\n",
    "\r\n",
    "  if x == 7:\r\n",
    "    start = timey.strftime('%d-%b')\r\n",
    "\r\n",
    "  head += [ timey.strftime('%d-%b') ]\r\n",
    "  data += [ covid_info['data'][0]['cur_total'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a pandas dataframe with the predefined head and data\r\n",
    "\r\n",
    "sum all the cases, and also create a numpy array of range from start date and last date\r\n",
    "\r\n",
    "then reshape the cases as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = pandas.DataFrame([data], columns = head)\r\n",
    "dates = confirmed.keys()\r\n",
    "cases = []\r\n",
    "\r\n",
    "for i in dates:\r\n",
    "    cases.append(confirmed[i].sum())\r\n",
    "\r\n",
    "days_since = numpy.array([i for i in range(len(dates))]).reshape(-1, 1)\r\n",
    "cases = numpy.array(cases).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define how many prediction days you want \r\n",
    "\r\n",
    "create a numpy array of the future date forecast\r\n",
    "\r\n",
    "also define a var for train and test data\r\n",
    "\r\n",
    "split the data to predefined var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_future = 3\r\n",
    "future_forcast = numpy.array([i for i in range(len(dates)+days_in_future)]).reshape(-1, 1)\r\n",
    "\r\n",
    "start_date = datetime.datetime.strptime(start, '%d-%b')\r\n",
    "future_forcast_dates = []\r\n",
    "cases_date = []\r\n",
    "for i in range(len(future_forcast)):\r\n",
    "future_forcast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%d-%b'))\r\n",
    "\r\n",
    "X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(days_since, cases, test_size=0.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a variable for svm predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['poly', 'sigmoid', 'rbf']\r\n",
    "c = [0.01, 0.1, 1, 10]\r\n",
    "gamma = [0.01, 0.1, 1]\r\n",
    "epsilon = [0.01, 0.1, 1]\r\n",
    "shrinking = [True, False]\r\n",
    "svm_grid = {'kernel': kernel, 'C': c, 'gamma' : gamma, 'epsilon': epsilon, 'shrinking' : shrinking}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a vsm prediction object and create a predictions with the train data\r\n",
    "\r\n",
    "collect the best prediction and finally predict the future covid-19 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVR()\r\n",
    "svm_search = RandomizedSearchCV(svm, svm_grid, scoring='neg_mean_squared_error', cv=3, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1)\r\n",
    "svm_search.fit(X_train_confirmed, y_train_confirmed.ravel())\r\n",
    "\r\n",
    "svm_search.best_params_\r\n",
    "\r\n",
    "svm_confirmed = svm_search.best_estimator_\r\n",
    "svm_pred = svm_confirmed.predict(future_forcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we use telegram as our wrapper, we cant use matplotlib to display the chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead we use __image-charts.com__ web embed image API service's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we need to reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ''\r\n",
    "y = ''\r\n",
    "for i in cases:\r\n",
    "    x += f'{round(i[0])},'\r\n",
    "for i in svm_pred:\r\n",
    "    y += f'{round(i)},'\r\n",
    "x = x.rstrip(',')\r\n",
    "y = y.rstrip(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we can output it as a url and the image will displayed to telegram as a embed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return f\"https://image-charts.com/chart?cht=lc&chd=a:|{ x }|{ y }&chdl=Prediksi|Positif&chxl=0:|{ '|'.join(future_forcast_dates) }|1:||1000|2500|5000|&chs=900x500&chco=3072F3,ff0000&chdlp=t&chls=2,4,1&chm=s,000000,0,-1,5|s,000000,1,-1,5&chxt=x,y\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python397jvsc74a57bd08da01e5a71448ea74f54d88afa8911010d1d12e23bc7e103d40d5def4a09152c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "8da01e5a71448ea74f54d88afa8911010d1d12e23bc7e103d40d5def4a09152c"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}